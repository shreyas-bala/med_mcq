{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install absl-py\n",
    "!pip install accelerate\n",
    "!pip install aiohappyeyeballs\n",
    "!pip install aiohttp\n",
    "!pip install aiosignal\n",
    "!pip install annotated-types\n",
    "!pip install anyio\n",
    "!pip install argon2-cffi\n",
    "!pip install argon2-cffi-bindings\n",
    "!pip install arrow\n",
    "!pip install asttokens\n",
    "!pip install astunparse\n",
    "!pip install async-lru\n",
    "!pip install attrs\n",
    "!pip install babel\n",
    "!pip install beautifulsoup4\n",
    "!pip install bitsandbytes\n",
    "!pip install bleach\n",
    "!pip install Brotli\n",
    "!pip install certifi\n",
    "!pip install cffi\n",
    "!pip install charset-normalizer\n",
    "!pip install comm\n",
    "!pip install contourpy\n",
    "!pip install cut-cross-entropy\n",
    "!pip install cycler\n",
    "!pip install datasets\n",
    "!pip install debugpy\n",
    "!pip install decorator\n",
    "!pip install defusedxml\n",
    "!pip install diffusers\n",
    "!pip install dill\n",
    "!pip install distro\n",
    "!pip install docstring_parser\n",
    "!pip install executing\n",
    "!pip install fastjsonschema\n",
    "!pip install filelock\n",
    "!pip install flatbuffers\n",
    "!pip install fonttools\n",
    "!pip install fqdn\n",
    "!pip install frozenlist\n",
    "!pip install fsspec\n",
    "!pip install gast\n",
    "!pip install google-pasta\n",
    "!pip install grpcio\n",
    "!pip install h11\n",
    "!pip install h5py\n",
    "!pip install hf_transfer\n",
    "!pip install hf-xet\n",
    "!pip install httpcore\n",
    "!pip install httpx\n",
    "!pip install huggingface-hub\n",
    "!pip install idna\n",
    "!pip install imageio\n",
    "!pip install importlib_metadata\n",
    "!pip install inflate64\n",
    "!pip install ipykernel\n",
    "!pip install ipython\n",
    "!pip install ipython_pygments_lexers\n",
    "!pip install ipywidgets\n",
    "!pip install isoduration\n",
    "!pip install jedi\n",
    "!pip install Jinja2\n",
    "!pip install jiter\n",
    "!pip install joblib\n",
    "!pip install json5\n",
    "!pip install jsonlines\n",
    "!pip install jsonpointer\n",
    "!pip install jsonschema\n",
    "!pip install jsonschema-specifications\n",
    "!pip install jupyter\n",
    "!pip install jupyter_client\n",
    "!pip install jupyter-console\n",
    "!pip install jupyter_core\n",
    "!pip install jupyter-events\n",
    "!pip install jupyter-lsp\n",
    "!pip install jupyter_server\n",
    "!pip install jupyter_server_terminals\n",
    "!pip install jupyterlab\n",
    "!pip install jupyterlab_pygments\n",
    "!pip install jupyterlab_server\n",
    "!pip install jupyterlab_widgets\n",
    "!pip install keras\n",
    "!pip install kiwisolver\n",
    "!pip install lazy_loader\n",
    "!pip install libclang\n",
    "!pip install Markdown\n",
    "!pip install markdown-it-py\n",
    "!pip install MarkupSafe\n",
    "!pip install matplotlib\n",
    "!pip install matplotlib-inline\n",
    "!pip install mdurl\n",
    "!pip install mistune\n",
    "!pip install ml_dtypes\n",
    "!pip install mpmath\n",
    "!pip install msgspec\n",
    "!pip install multidict\n",
    "!pip install multiprocess\n",
    "!pip install multivolumefile\n",
    "!pip install namex\n",
    "!pip install nbclient\n",
    "!pip install nbconvert\n",
    "!pip install nbformat\n",
    "!pip install nest-asyncio\n",
    "!pip install networkx\n",
    "!pip install notebook\n",
    "!pip install notebook_shim\n",
    "!pip install numpy\n",
    "!pip install nvidia-cublas-cu11\n",
    "!pip install nvidia-cublas-cu12\n",
    "!pip install nvidia-cuda-cupti-cu11\n",
    "!pip install nvidia-cuda-cupti-cu12\n",
    "!pip install nvidia-cuda-nvrtc-cu11\n",
    "!pip install nvidia-cuda-nvrtc-cu12\n",
    "!pip install nvidia-cuda-runtime-cu11\n",
    "!pip install nvidia-cuda-runtime-cu12\n",
    "!pip install nvidia-cudnn-cu11\n",
    "!pip install nvidia-cudnn-cu12\n",
    "!pip install nvidia-cufft-cu11\n",
    "!pip install nvidia-cufft-cu12\n",
    "!pip install nvidia-cufile-cu12\n",
    "!pip install nvidia-curand-cu11\n",
    "!pip install nvidia-curand-cu12\n",
    "!pip install nvidia-cusolver-cu11\n",
    "!pip install nvidia-cusolver-cu12\n",
    "!pip install nvidia-cusparse-cu11\n",
    "!pip install nvidia-cusparse-cu12\n",
    "!pip install nvidia-cusparselt-cu12\n",
    "!pip install nvidia-nccl-cu11\n",
    "!pip install nvidia-nccl-cu12\n",
    "!pip install nvidia-nvjitlink-cu12\n",
    "!pip install nvidia-nvtx-cu11\n",
    "!pip install nvidia-nvtx-cu12\n",
    "!pip install openai\n",
    "!pip install opencv-python\n",
    "!pip install opt_einsum\n",
    "!pip install optree\n",
    "!pip install overrides\n",
    "!pip install packaging\n",
    "!pip install pandas\n",
    "!pip install pandocfilters\n",
    "!pip install parso\n",
    "!pip install peft\n",
    "!pip install pexpect\n",
    "!pip install pillow\n",
    "!pip install pip\n",
    "!pip install platformdirs\n",
    "!pip install prometheus_client\n",
    "!pip install prompt_toolkit\n",
    "!pip install propcache\n",
    "!pip install protobuf\n",
    "!pip install psutil\n",
    "!pip install ptyprocess\n",
    "!pip install pure_eval\n",
    "!pip install py7zr\n",
    "!pip install pyarrow\n",
    "!pip install pybcj\n",
    "!pip install pycparser\n",
    "!pip install pycryptodomex\n",
    "!pip install pydantic\n",
    "!pip install pydantic_core\n",
    "!pip install Pygments\n",
    "!pip install pyparsing\n",
    "!pip install pyppmd\n",
    "!pip install python-dateutil\n",
    "!pip install python-json-logger\n",
    "!pip install pytorch-msssim\n",
    "!pip install pytorch_ssim\n",
    "!pip install pytz\n",
    "!pip install PyYAML\n",
    "!pip install pyzmq\n",
    "!pip install pyzstd\n",
    "!pip install referencing\n",
    "!pip install regex\n",
    "!pip install requests\n",
    "!pip install rfc3339-validator\n",
    "!pip install rfc3986-validator\n",
    "!pip install rich\n",
    "!pip install rpds-py\n",
    "!pip install safetensors\n",
    "!pip install scikit-image\n",
    "!pip install scikit-learn\n",
    "!pip install scipy\n",
    "!pip install Send2Trash\n",
    "!pip install sentencepiece\n",
    "!pip install setuptools\n",
    "!pip install shtab\n",
    "!pip install six\n",
    "!pip install sniffio\n",
    "!pip install soupsieve\n",
    "!pip install stack-data\n",
    "!pip install sympy\n",
    "!pip install tensorboard\n",
    "!pip install tensorboard-data-server\n",
    "!pip install tensorflow\n",
    "!pip install termcolor\n",
    "!pip install terminado\n",
    "!pip install texttable\n",
    "!pip install tf_keras\n",
    "!pip install threadpoolctl\n",
    "!pip install tifffile\n",
    "!pip install timm\n",
    "!pip install tinycss2\n",
    "!pip install tokenizers\n",
    "!pip install torch\n",
    "!pip install torchaudio\n",
    "!pip install torchvision\n",
    "!pip install tornado\n",
    "!pip install tqdm\n",
    "!pip install traitlets\n",
    "!pip install transformers\n",
    "!pip install triton\n",
    "!pip install trl\n",
    "!pip install typeguard\n",
    "!pip install types-python-dateutil\n",
    "!pip install typing_extensions\n",
    "!pip install typing-inspection\n",
    "!pip install tyro\n",
    "!pip install tzdata\n",
    "!pip install unsloth\n",
    "!pip install unsloth_zoo\n",
    "!pip install uri-template\n",
    "!pip install urllib3\n",
    "!pip install wcwidth\n",
    "!pip install webcolors\n",
    "!pip install webencodings\n",
    "!pip install websocket-client\n",
    "!pip install Werkzeug\n",
    "!pip install wheel\n",
    "!pip install widgetsnbextension\n",
    "!pip install wrapt\n",
    "!pip install xformers\n",
    "!pip install xxhash\n",
    "!pip install yarl\n",
    "!pip install zipp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from transformers import TrainingArguments as HFTrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainerCallback\n",
    "import random\n",
    "\n",
    "# 🚀 GPU Setup\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3,4,5,6,7\"\n",
    "device_map = \"auto\"\n",
    "\n",
    "# ⚡ Model Configuration - Optimized for reasoning\n",
    "max_seq_length = 1024  # Keep your original\n",
    "load_in_4bit = True\n",
    "dtype = torch.float16 if torch.cuda.get_device_capability(0)[0] == 7 else None\n",
    "\n",
    "# 🎯 Load Model with device mapping\n",
    "device_map = \"auto\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"microsoft/Phi-3.5-mini-instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "# 🎯 Apply Phi-3 formatting template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"phi-3\",\n",
    "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
    ")\n",
    "\n",
    "# 📊 Load your reasoning dataset\n",
    "def load_reasoning_dataset(path):\n",
    "    \"\"\"Load your final_train_with_reasoning.jsonl file\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        return [json.loads(line.strip()) for line in f]\n",
    "\n",
    "print(\"📊 Loading reasoning dataset...\")\n",
    "reasoning_data = load_reasoning_dataset(\"medmcqa_structured_formatted.jsonl\")\n",
    "print(f\"✅ Loaded {len(reasoning_data)} samples with reasoning\")\n",
    "\n",
    "# 🎯 ENHANCED FORMATTING with motivation and reasoning emphasis\n",
    "def enhance_conversation_for_reasoning(sample):\n",
    "    \"\"\"Enhance the conversation to leverage reasoning better\"\"\"\n",
    "    \n",
    "    # Extract current conversation\n",
    "    human_msg = sample[\"messages\"][0][\"value\"]\n",
    "    gpt_msg = sample[\"messages\"][1][\"value\"]\n",
    "    \n",
    "    # 🔥 Add motivation and better instruction to human message\n",
    "    enhanced_human = f\"\"\"You are an expert medical AI assistant specialized in clinical MCQ prediction with exceptional reasoning abilities.\n",
    "\n",
    "Your task: Analyze the clinical question carefully, apply your medical knowledge, and provide the correct answer with clear reasoning.\n",
    "\n",
    "{human_msg}\n",
    "\n",
    "Provide your answer as: [LETTER]\n",
    "Then explain your reasoning clearly.\"\"\"\n",
    "    \n",
    "    # 🔥 Enhance GPT response format for better learning\n",
    "    # Extract answer letter and reasoning\n",
    "    answer_letter = gpt_msg.split(\"Reasoning:\")[0].strip()\n",
    "    reasoning_part = gpt_msg.split(\"Reasoning:\")[-1].strip() if \"Reasoning:\" in gpt_msg else \"\"\n",
    "    \n",
    "    enhanced_gpt = f\"\"\"{answer_letter}\n",
    "\n",
    "Reasoning: {reasoning_part}\"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"id\": sample.get(\"id\", \"unused\"),\n",
    "        \"messages\": [\n",
    "            {\"from\": \"human\", \"value\": enhanced_human},\n",
    "            {\"from\": \"gpt\", \"value\": enhanced_gpt}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# 🚀 Apply enhancement to dataset\n",
    "print(\"🔄 Enhancing dataset for reasoning...\")\n",
    "enhanced_data = [enhance_conversation_for_reasoning(sample) for sample in reasoning_data]\n",
    "\n",
    "# 🎯 Additional data augmentation for hard questions (A-J support)\n",
    "def add_hard_question_examples(data):\n",
    "    \"\"\"Add synthetic examples to help model understand A-J options\"\"\"\n",
    "    \n",
    "    hard_examples = [\n",
    "        {\n",
    "            \"id\": \"synthetic_1\",\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"from\": \"human\", \n",
    "                    \"value\": \"\"\"You are an expert medical AI assistant specialized in clinical MCQ prediction with exceptional reasoning abilities.\n",
    "\n",
    "Your task: Analyze the clinical question carefully, apply your medical knowledge, and provide the correct answer with clear reasoning.\n",
    "\n",
    "Question: Which of the following is a contraindication for MRI?\n",
    "Options: A. Pregnancy B. Claustrophobia C. Pacemaker D. Diabetes E. Hypertension F. Asthma G. Obesity H. Age >65 I. Previous surgery J. Allergies\n",
    "Answer with the correct option letter only.\n",
    "\n",
    "Provide your answer as: [LETTER]\n",
    "Then explain your reasoning clearly.\"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": \"\"\"C\n",
    "\n",
    "Reasoning: Cardiac pacemakers are an absolute contraindication for MRI due to the strong magnetic field which can interfere with pacemaker function and potentially cause life-threatening arrhythmias.\"\"\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"synthetic_2\", \n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": \"\"\"You are an expert medical AI assistant specialized in clinical MCQ prediction with exceptional reasoning abilities.\n",
    "\n",
    "Your task: Analyze the clinical question carefully, apply your medical knowledge, and provide the correct answer with clear reasoning.\n",
    "\n",
    "Question: First-line treatment for acute myocardial infarction includes:\n",
    "Options: A. Antibiotics B. Steroids C. Aspirin D. Paracetamol E. Insulin F. Oxygen G. Morphine H. Nitroglycerin I. Heparin J. All of the above except A,B,D,E\n",
    "Answer with the correct option letter only.\n",
    "\n",
    "Provide your answer as: [LETTER]\n",
    "Then explain your reasoning clearly.\"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\", \n",
    "                    \"value\": \"\"\"J\n",
    "\n",
    "Reasoning: Acute MI treatment includes aspirin (antiplatelet), oxygen (if hypoxic), morphine (pain relief), nitroglycerin (vasodilation), and heparin (anticoagulation). Antibiotics, steroids, paracetamol, and insulin are not first-line treatments.\"\"\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return data + hard_examples\n",
    "\n",
    "# Add hard question examples\n",
    "enhanced_data = add_hard_question_examples(enhanced_data)\n",
    "print(f\"✅ Enhanced dataset size: {len(enhanced_data)}\")\n",
    "\n",
    "# 🎯 Convert to HuggingFace dataset\n",
    "hf_dataset = Dataset.from_list(enhanced_data)\n",
    "\n",
    "# 🚀 OPTIMIZED tokenization for reasoning\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Optimized tokenization that preserves reasoning structure\"\"\"\n",
    "    convos = examples[\"messages\"]\n",
    "    texts = []\n",
    "    for convo in convos:\n",
    "        # Apply chat template with careful formatting\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            convo, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "hf_dataset = hf_dataset.map(formatting_prompts_func, batched=True, num_proc=64)\n",
    "\n",
    "# 📊 Smart train/validation split\n",
    "split_dataset = hf_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "split_dataset = DatasetDict({\n",
    "    \"train\": split_dataset[\"train\"],\n",
    "    \"validation\": split_dataset[\"test\"]\n",
    "})\n",
    "\n",
    "print(f\"📊 Train: {len(split_dataset['train'])}, Val: {len(split_dataset['validation'])}\")\n",
    "\n",
    "# 🔧 OPTIMIZED LoRA for reasoning\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # Increased for better reasoning capacity\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,  # Keep low for reasoning stability\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    "    use_rslora=True,  # Better for reasoning tasks\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "# 🎯 ACCURACY-FOCUSED training arguments\n",
    "training_args = HFTrainingArguments(\n",
    "    output_dir=\"outputs_reasoning_enhanced\",\n",
    "    per_device_train_batch_size=12,  # Optimized for reasoning\n",
    "    per_device_eval_batch_size=12,\n",
    "    gradient_accumulation_steps=2,   # Effective batch size = 24\n",
    "    num_train_epochs=2,              # Sufficient for reasoning dataset\n",
    "    logging_steps=20,\n",
    "    learning_rate=2e-4,              # Higher for reasoning adaptation\n",
    "    warmup_ratio=0.1,\n",
    "    warmup_steps=50,\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    seed=42,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    dataloader_num_workers=4,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=None,\n",
    "    #dataloader_shuffle=True,         # Important for reasoning diversity\n",
    ")\n",
    "\n",
    "# 🎯 Enhanced callback for monitoring\n",
    "class ReasoningTrainingCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.best_loss = float('inf')\n",
    "        self.step_losses = []\n",
    "        \n",
    "    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n",
    "        if logs:\n",
    "            if \"train_loss\" in logs:\n",
    "                self.step_losses.append(logs[\"train_loss\"])\n",
    "                if len(self.step_losses) % 50 == 0:\n",
    "                    avg_loss = sum(self.step_losses[-50:]) / 50\n",
    "                    print(f\"📊 Step {state.global_step}: Avg Loss (last 50) = {avg_loss:.4f}\")\n",
    "            \n",
    "            if \"eval_loss\" in logs:\n",
    "                eval_loss = logs[\"eval_loss\"]\n",
    "                if eval_loss < self.best_loss:\n",
    "                    self.best_loss = eval_loss\n",
    "                    print(f\"🎯 NEW BEST: Eval Loss = {eval_loss:.4f} at step {state.global_step}\")\n",
    "\n",
    "# 🚀 Initialize enhanced trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=split_dataset[\"train\"],\n",
    "    eval_dataset=split_dataset[\"validation\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=128,\n",
    "    packing=False,  # Critical for reasoning preservation\n",
    "    args=training_args,\n",
    "    callbacks=[ReasoningTrainingCallback()],\n",
    ")\n",
    "\n",
    "# 🎯 ENHANCED TRAINING with reasoning focus\n",
    "print(\"🚀 Starting reasoning-enhanced training for MAXIMUM ACCURACY...\")\n",
    "print(\"🎯 Training with enhanced prompts, reasoning focus, and A-J support\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 💾 Save the reasoning-enhanced model\n",
    "print(\"💾 Saving reasoning-enhanced model...\")\n",
    "model.save_pretrained(\"lora_model_shre_has_vpn_issues\")\n",
    "tokenizer.save_pretrained(\"lora_model_shre_has_vpn_issues\")\n",
    "\n",
    "print(\"✅ REASONING-ENHANCED TRAINING COMPLETED!\")\n",
    "print(\"🎯 Model saved to 'lora_model_reasoning_enhanced'\")\n",
    "print(\"📊 Expected accuracy improvement: +0.20-0.35 from reasoning enhancement\")\n",
    "\n",
    "# 🧪 Quick reasoning test\n",
    "print(\"\\n🧪 Testing reasoning capability...\")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "test_prompt = \"\"\"You are an expert medical AI assistant specialized in clinical MCQ prediction with exceptional reasoning abilities.\n",
    "\n",
    "Your task: Analyze the clinical question carefully, apply your medical knowledge, and provide the correct answer with clear reasoning.\n",
    "\n",
    "Question: A 45-year-old presents with sudden onset severe headache. CT shows subarachnoid hemorrhage. Most likely cause?\n",
    "Options: A. Hypertension B. Aneurysm rupture C. Trauma D. Arteriovenous malformation E. Moyamoya disease\n",
    "Answer with the correct option letter only.\n",
    "\n",
    "Provide your answer as: [LETTER]\n",
    "Then explain your reasoning clearly.\"\"\"\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=150, \n",
    "        do_sample=False, \n",
    "        temperature=0.1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"🔍 Model Response:\\n{response.split('Provide your answer as:')[-1].strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import GenerationConfig\n",
    "from collections import Counter\n",
    "\n",
    "# ===== MODEL SETUP =====\n",
    "model_name = \"lora_model_trainedonmedmcqa\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=1024,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"phi-3\",\n",
    "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# ===== FORMATTING =====\n",
    "def format_direct_prompt(item):\n",
    "    q = item[\"question\"]\n",
    "    opts = item[\"options\"]\n",
    "    opt_str = \"\\n\".join([f\"{k}. {opts[k]}\" for k in sorted(opts)])\n",
    "    return f\"{q}\\n\\n{opt_str}\\n\\nAnswer with the correct option only.\"\n",
    "\n",
    "def format_reasoning_prompt(item):\n",
    "    q = item[\"question\"]\n",
    "    opts = item[\"options\"]\n",
    "    opt_str = \"\\n\".join([f\"{k}. {opts[k]}\" for k in sorted(opts)])\n",
    "    return (\n",
    "        f\"{q}\\n\\n\"\n",
    "        f\"{opt_str}\\n\\n\"\n",
    "        \"Please reason through the options and then give your answer in the following format:\\n\"\n",
    "        \"Explanation: <your explanation>\\n\"\n",
    "        \"Answer: <correct option letter>) <option text>\"\n",
    "    )\n",
    "\n",
    "# ===== UTILS =====\n",
    "def extract_last_letter(outputs):\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    for char in reversed(decoded.strip().upper()):\n",
    "        if char in \"ABCDEFGHIJ\":\n",
    "            return char\n",
    "    return None\n",
    "\n",
    "def generate_answer(prompt, temp=0.7):\n",
    "    messages = [{\"from\": \"human\", \"value\": prompt}]\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        return_tensors=\"pt\",\n",
    "        add_generation_prompt=True\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=3,\n",
    "        do_sample=True,\n",
    "        temperature=temp,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "    return extract_last_letter(outputs)\n",
    "\n",
    "def get_reasoning_and_answer(messages, options):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    output_text = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "    explanation = \"\"\n",
    "    answer_letter = None\n",
    "    answer_text = None\n",
    "\n",
    "    for line in output_text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if line.lower().startswith(\"explanation:\"):\n",
    "            explanation = line[len(\"explanation:\"):].strip()\n",
    "        elif line.lower().startswith(\"answer:\"):\n",
    "            answer_part = line[len(\"answer:\"):].strip()\n",
    "            if \")\" in answer_part:\n",
    "                parts = answer_part.split(\")\", 1)\n",
    "                answer_letter = parts[0].strip().upper()\n",
    "                answer_text = parts[1].strip()\n",
    "            elif answer_part[:1] in options:\n",
    "                answer_letter = answer_part[:1].upper()\n",
    "                answer_text = options.get(answer_letter, \"\")\n",
    "    return explanation, answer_letter, answer_text\n",
    "\n",
    "# ===== LOAD DATA =====\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "easy_data = load_jsonl(\"test_easy.jsonl\")\n",
    "medium_data = load_jsonl(\"test_medium.jsonl\")\n",
    "hard_data = load_jsonl(\"test_hard.jsonl\")\n",
    "\n",
    "# ===== MAIN EXECUTION =====\n",
    "results = []\n",
    "\n",
    "# Easy and Medium → Direct\n",
    "for subset, label in zip([easy_data, medium_data], [\"Easy\", \"Medium\"]):\n",
    "    for item in tqdm(subset, desc=label):\n",
    "        prompt = format_direct_prompt(item)\n",
    "        answer = generate_answer(prompt, temp=0.3)\n",
    "        results.append({\n",
    "            \"id\": item[\"id\"],\n",
    "            \"answer\": answer or \"?\",\n",
    "            \"explanation\": \"\"\n",
    "        })\n",
    "\n",
    "# Hard → Reasoned\n",
    "for item in tqdm(hard_data, desc=\"Hard\"):\n",
    "    prompt = format_reasoning_prompt(item)\n",
    "    messages = [{\"from\": \"human\", \"value\": prompt}]\n",
    "    explanation, answer_letter, answer_text = get_reasoning_and_answer(messages, item[\"options\"])\n",
    "    results.append({\n",
    "        \"id\": item[\"id\"],\n",
    "        \"answer\": f\"{answer_letter})\" if answer_letter else \"?\",\n",
    "        \"explanation\": explanation\n",
    "    })\n",
    "\n",
    "# ===== SAVE =====\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"submission_medmcqa_direct+reason.csv\", index=False)\n",
    "print(\"✅ Saved to submission_stratified_reasoning.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
